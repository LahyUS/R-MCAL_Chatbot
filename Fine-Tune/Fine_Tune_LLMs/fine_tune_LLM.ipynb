{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "49811c53-49e7-4996-9668-e1de1e5042c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\numpy\\_distributor_init.py:30: UserWarning: loaded more than 1 DLL from .libs:\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.21-gcc_10_3_0.dll\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\numpy\\.libs\\libopenblas64__v0.3.23-246-g3d31191b-gcc_10_3_0.dll\n",
      "  warnings.warn(\"loaded more than 1 DLL from .libs:\"\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#*********************************************************************************************************\n",
    "#**                                                Import                                               **\n",
    "#*********************************************************************************************************\n",
    "from datasets import load_dataset, DatasetDict, Dataset, load_from_disk\n",
    "from peft import PeftModel, PeftConfig, get_peft_model, LoraConfig\n",
    "import evaluate\n",
    "import torch\n",
    "import numpy as np\n",
    "import os\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoConfig, \n",
    "    AutoModelForSequenceClassification,\n",
    "    DataCollatorWithPadding,\n",
    "    TrainingArguments,\n",
    "    Trainer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "96ceff4c-eec5-478b-89ca-1eeb1f33cb24",
   "metadata": {},
   "outputs": [],
   "source": [
    "lets_train = False\n",
    "\n",
    "model_checkpoint = \"C:\\\\Users\\\\rvc_sw_mss1_common\\\\Desktop\\\\test\\\\mcal-guru\\\\model\\\\distilbert-base-uncased\"\n",
    "dataset_directory = \"C:\\\\Users\\\\rvc_sw_mss1_common\\\\Desktop\\\\test\\\\mcal-guru\\\\Fine_Tune\\\\Datasets\\\\Saved_datasets\\\\shawhin-imdb-truncated\"\n",
    "dataset_name = 'shawhin/imdb-truncated'\n",
    "metrics_dir = \"C:\\\\Users\\\\rvc_sw_mss1_common\\\\Desktop\\\\test\\\\mcal-guru\\\\Fine_Tune\\\\evaluate\\\\metrics\\\\accuracy\\\\accuracy.py\"\n",
    "model_save_path = \"C:\\\\Users\\\\rvc_sw_mss1_common\\\\Desktop\\\\test\\\\mcal-guru\\\\Fine_Tune\\\\Trained_models\\\\distilbert-base-uncased\\\\\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "737e23a8-21af-41a0-a7db-2782dd796ddd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\mcal-guru\\model\\distilbert-base-uncased and are newly initialized: ['classifier.bias', 'pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "#*********************************************************************************************************\n",
    "#**                                                Base Model                                           **\n",
    "#*********************************************************************************************************\n",
    "# define label maps\n",
    "id2label = {0: \"Negative\", 1: \"Positive\"}\n",
    "label2id = {\"Negative\":0, \"Positive\":1}\n",
    "\n",
    "# generate classification model from model_checkpoint\n",
    "model = AutoModelForSequenceClassification.from_pretrained(\n",
    "    model_checkpoint, num_labels=2, id2label=id2label, label2id=label2id)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5d64efa3-cecc-496d-b40d-bcd3a5180e20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset already downloaded. Loading from disk\n",
      "Dataset has been loaded\n",
      "DatasetDict({\n",
      "    train: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "    validation: Dataset({\n",
      "        features: ['label', 'text'],\n",
      "        num_rows: 1000\n",
      "    })\n",
      "})\n"
     ]
    }
   ],
   "source": [
    "#*********************************************************************************************************\n",
    "#**                                                Load Data                                            **\n",
    "#*********************************************************************************************************\n",
    "# load dataset\n",
    "\n",
    "if os.path.exists(dataset_directory) and os.listdir(dataset_directory):\n",
    "    print(\"Dataset already downloaded. Loading from disk\")\n",
    "    dataset = load_from_disk(dataset_directory)\n",
    "    print(\"Dataset has been loaded\")\n",
    "else:\n",
    "    print(\"Downloading dataset...\")\n",
    "    dataset = load_dataset(dataset_name)\n",
    "    print(\"Saving dataset to disk...\")\n",
    "    dataset.save_to_disk(dataset_directory)\n",
    "    \n",
    "print(dataset)\n",
    "\n",
    "# dataset = \n",
    "# DatasetDict({\n",
    "#     train: Dataset({\n",
    "#         features: ['label', 'text'],\n",
    "#         num_rows: 1000\n",
    "#     })\n",
    "#     validation: Dataset({\n",
    "#         features: ['label', 'text'],\n",
    "#         num_rows: 1000\n",
    "#     })\n",
    "# }) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7c1b869f-0aca-4e71-b8b6-ab9a2c3c4808",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********************************************************************************************************\n",
    "#**                                                Preprocess Data                                      **\n",
    "#*********************************************************************************************************\n",
    "# create tokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_checkpoint, add_prefix_space=True)\n",
    "\n",
    "# create tokenize function\n",
    "def tokenize_function(examples):\n",
    "    # extract text\n",
    "    text = examples[\"text\"]\n",
    "\n",
    "    #tokenize and truncate text\n",
    "    tokenizer.truncation_side = \"left\"\n",
    "    tokenized_inputs = tokenizer(\n",
    "        text,\n",
    "        return_tensors=\"np\",\n",
    "        truncation=True,\n",
    "        max_length=512\n",
    "    )\n",
    "\n",
    "    return tokenized_inputs\n",
    "\n",
    "# add pad token if none exists\n",
    "if tokenizer.pad_token is None:\n",
    "    tokenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
    "    model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "# tokenize training and validation datasets\n",
    "tokenized_dataset = dataset.map(tokenize_function, batched=True)\n",
    "tokenized_dataset\n",
    "\n",
    "# tokenized_dataset = \n",
    "# DatasetDict({\n",
    "#     train: Dataset({\n",
    "#        features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
    "#         num_rows: 1000\n",
    "#     })\n",
    "#     validation: Dataset({\n",
    "#         features: ['label', 'text', 'input_ids', 'attention_mask'],\n",
    "#         num_rows: 1000\n",
    "#     })\n",
    "# })\n",
    "\n",
    "# create data collator\n",
    "data_collator = DataCollatorWithPadding(tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6ad58727-c920-4222-b205-c02bd4ea2bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#*********************************************************************************************************\n",
    "#**                                                Evaluation Metrics                                   **\n",
    "#*********************************************************************************************************\n",
    "# import accuracy evaluation metric\n",
    "accuracy = evaluate.load(metrics_dir)\n",
    "\n",
    "# define an evaluation function to pass into trainer later\n",
    "def compute_metrics(p):\n",
    "    predictions, labels = p\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "\n",
    "    return {\"accuracy\": accuracy.compute(predictions=predictions, \n",
    "                                          references=labels)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d37b25be-99ce-422f-a906-2bdac3151516",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Untrained model predictions:\n",
      "----------------------------\n",
      "It was good. - Positive\n",
      "Not a fan, don't recommed. - Positive\n",
      "Better than the first one. - Positive\n",
      "This is not worth watching even once. - Positive\n",
      "This one is a pass. - Positive\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#*********************************************************************************************************\n",
    "#**                                                Untrained Model Performance                          **\n",
    "#*********************************************************************************************************\n",
    "# define list of examples\n",
    "text_list = [\"It was good.\", \"Not a fan, don't recommed.\", \n",
    "\"Better than the first one.\", \"This is not worth watching even once.\", \n",
    "\"This one is a pass.\"]\n",
    "\n",
    "print(\"Untrained model predictions:\")\n",
    "print(\"----------------------------\")\n",
    "for text in text_list:\n",
    "    # tokenize text\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\")\n",
    "    # compute logits\n",
    "    logits = model(inputs).logits\n",
    "    # convert logits to label\n",
    "    predictions = torch.argmax(logits)\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()])\n",
    "\n",
    "# Output:\n",
    "# Untrained model predictions:\n",
    "# ----------------------------\n",
    "# It was good. - Negative\n",
    "# Not a fan, don't recommed. - Negative\n",
    "# Better than the first one. - Negative\n",
    "# This is not worth watching even once. - Negative\n",
    "# This one is a pass. - Negative"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "732c1c3f-5871-4ed1-bed2-6b8217c0e7b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 1,221,124 || all params: 67,584,004 || trainable%: 1.8068239934408148\n"
     ]
    }
   ],
   "source": [
    "#*********************************************************************************************************\n",
    "#**                                                Fine-tuning with LoRA                                **\n",
    "#*********************************************************************************************************\n",
    "peft_config = LoraConfig(task_type=\"SEQ_CLS\", # sequence classification\n",
    "                        r=4, # intrinsic rank of trainable weight matrix\n",
    "                        lora_alpha=32, # this is like a learning rate\n",
    "                        lora_dropout=0.01, # probablity of dropout\n",
    "                        target_modules = ['q_lin']) # we apply lora to query layer only\n",
    "\n",
    "\n",
    "model = get_peft_model(model, peft_config)\n",
    "model.print_trainable_parameters()\n",
    "\n",
    "# trainable params: 1,221,124 || all params: 67,584,004 || trainable%: 1.8068239934408148\n",
    "\n",
    "\n",
    "# hyperparameters\n",
    "lr = 1e-3 # size of optimization step \n",
    "batch_size = 4 # number of examples processed per optimziation step\n",
    "num_epochs = 10 # number of times model runs through training data\n",
    "\n",
    "# define training arguments\n",
    "training_args = TrainingArguments(\n",
    "    output_dir= model_checkpoint + \"-lora-text-classification\",\n",
    "    learning_rate=lr,\n",
    "    per_device_train_batch_size=batch_size, \n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=num_epochs,\n",
    "    weight_decay=0.01,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    load_best_model_at_end=True,\n",
    ")\n",
    "\n",
    "\n",
    "# creater trainer object\n",
    "trainer = Trainer(\n",
    "    model=model, # our peft model\n",
    "    args=training_args, # hyperparameters\n",
    "    train_dataset=tokenized_dataset[\"train\"], # training data\n",
    "    eval_dataset=tokenized_dataset[\"validation\"], # validation data\n",
    "    tokenizer=tokenizer, # define tokenizer\n",
    "    data_collator=data_collator, # this will dynamically pad examples in each batch to be equal length\n",
    "    compute_metrics=compute_metrics, # evaluates model using compute_metrics() function from before\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "2c215f00-4d31-4bb6-8ddb-61e0155c5b5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a DistilBertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1250' max='1250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1250/1250 02:42, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.307818</td>\n",
       "      <td>{'accuracy': 0.879}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.391753</td>\n",
       "      <td>{'accuracy': 0.876}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>No log</td>\n",
       "      <td>0.406185</td>\n",
       "      <td>{'accuracy': 0.891}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.503213</td>\n",
       "      <td>{'accuracy': 0.893}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.611084</td>\n",
       "      <td>{'accuracy': 0.893}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.751565</td>\n",
       "      <td>{'accuracy': 0.882}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>0.279900</td>\n",
       "      <td>0.782382</td>\n",
       "      <td>{'accuracy': 0.891}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.793997</td>\n",
       "      <td>{'accuracy': 0.888}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.827744</td>\n",
       "      <td>{'accuracy': 0.885}</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>0.034000</td>\n",
       "      <td>0.833932</td>\n",
       "      <td>{'accuracy': 0.885}</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\nn\\parallel\\_functions.py:68: UserWarning: Was asked to gather along dimension 0, but all input tensors were scalars; will instead unsqueeze and return a vector.\n",
      "  warnings.warn('Was asked to gather along dimension 0, but all '\n",
      "C:\\Users\\rvc_sw_mss1_common\\Desktop\\test\\venv\\Lib\\site-packages\\torch\\cuda\\nccl.py:15: UserWarning: PyTorch is not compiled with NCCL support\n",
      "  warnings.warn('PyTorch is not compiled with NCCL support')\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1250, training_loss=0.12727512798309326, metrics={'train_runtime': 165.7268, 'train_samples_per_second': 60.34, 'train_steps_per_second': 7.543, 'total_flos': 1259299691321088.0, 'train_loss': 0.12727512798309326, 'epoch': 10.0})"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# train model\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5daaeb4a-c52a-48b6-8b97-3b45a3a8e463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " hey, do not disturb me\n",
      "\n",
      " are you crazy\n",
      "\n",
      " I love you so so much\n",
      "\n",
      " How can I do that?\n",
      "\n",
      " Boring\n",
      "\n",
      " Oh, what's next?\n"
     ]
    }
   ],
   "source": [
    "test_corpus = [\n",
    "    \"hey, do not disturb me\",\n",
    "    \"are you crazy\",\n",
    "    \"I love you so so much\",\n",
    "    \"How can I do that?\",\n",
    "    \"Boring\",\n",
    "    \"Oh, what's next?\"\n",
    "]\n",
    "\n",
    "for i in test_corpus:\n",
    "    print(\"\\n\", i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5c771ccc-320a-4040-ac7a-6c0fc3bc810a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trained model predictions:\n",
      "--------------------------\n",
      "hey, do not disturb me - Positive\n",
      "are you crazy - Negative\n",
      "I love you so so much - Positive\n",
      "How can I do that? - Negative\n",
      "Boring - Negative\n",
      "Oh, what's next? - Negative\n"
     ]
    }
   ],
   "source": [
    "#*********************************************************************************************************\n",
    "#**                                                Trained Model Performance                            **\n",
    "#*********************************************************************************************************\n",
    "model.to('cpu') # moving to mps for Mac (can alternatively do 'cpu')\n",
    "\n",
    "print(\"Trained model predictions:\")\n",
    "print(\"--------------------------\")\n",
    "for text in test_corpus:\n",
    "    inputs = tokenizer.encode(text, return_tensors=\"pt\").to(\"cpu\") # moving to mps for Mac (can alternatively do 'cpu')\n",
    "\n",
    "    logits = model(inputs).logits\n",
    "    predictions = torch.max(logits,1).indices\n",
    "\n",
    "    print(text + \" - \" + id2label[predictions.tolist()[0]])\n",
    "\n",
    "# Output:\n",
    "# Trained model predictions:\n",
    "# ----------------------------\n",
    "# It was good. - Positive\n",
    "# Not a fan, don't recommed. - Negative\n",
    "# Better than the first one. - Positive\n",
    "# This is not worth watching even once. - Negative\n",
    "# This one is a pass. - Positive # this one is tricky"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e99305-d2e8-4497-91f8-2932c37bfb7a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
